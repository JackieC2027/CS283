1. Your shell forks multiple child processes when executing piped commands. How does your implementation ensure that all child processes complete before the shell continues accepting user input? What would happen if you forgot to call waitpid() on all child processes?

Within the implementation of the shell, all of the child processes are ensured to complete before the shell can continue accepting user input through the built-in system call waitpid(). Although waitpid() usually waits for one child process to finish, this principle can be applied to an arrary of process IDs that are stored in memory that I looped through. For each referenced process ID, I can loop through each process ID in the array and allow the parent process to wait for each child process to conclude before accepting new commands, which is extremely important for piping in shell environments. If I were to forget to wait for all of the child processes to return some value before accepting more input, it is possible that the parent process can terminate before any of the child processes are finished, so distributed resources cannot return to the pool of process resources, leading to future limitations if too many zombie processes are allocated.

2. The dup2() function is used to redirect input and output file descriptors. Explain why it is necessary to close unused pipe ends after calling dup2(). What could go wrong if you leave pipes open?

It is necessary to close unused pipe ends after calling dup2() to prevent descriptor leaks, where prolonged distribution of these pipe resources, the program will eventually run out of available file descriptors. Since dup2() replaces the standard-in and standard-out file descriptors with the desginated descriptors, reading processes that are piped are not able to work properly, so the writing process would need to wait for the infinitely waiting reading process to finish, holding resources that cannot be freed and distributed to other file resources. If you were to leave pipes open, I believe the shell could become unresponsive and is not capable of accepting new requests until the bottleneck is fixed since there would not be enough resources to continously add more read/write file descriptors. 

3. Your shell recognizes built-in commands (cd, exit, dragon). Unlike external commands, built-in commands do not require execvp(). Why is cd implemented as a built-in rather than an external command? What challenges would arise if cd were implemented as an external process?

The reason that cd is implemented as a built-in function, instead of an external command, lies in how the processes are interacting with the working directories in different environments. Through the implementation of the cd command as a built-in, this would actually change the working directory of the Drexel shell process, which has a different process ID than running the system call outside of the Drexel shell environment. Instead, cd commands that are implemented as external commands, the creation of a new child process would be necessary, as the command needs to be forked, which would ultimately only change the working directory of the new child process, which has a different parent process ID. Although after the parent process waits for the child process to finish, the directory of the parent shell would not be changed, as if the cd commands never even happened. Some other issues that might arise with cd as an external process would be accessing the pwd for certain environemnts, as virtual shells on MacOS have a prefix of /private that might alter expected, which might even differ between Windows and other Linux distributions, as well as the underlying performance for the command, as you would to implement the fork/exec pattern and wait for its necessary child process to conclude before continuing, besides using a simple chdir() one liner to solve the same problem.

4. Currently, your shell supports a fixed number of piped commands (CMD_MAX). How would you modify your implementation to allow an arbitrary number of piped commands while still handling memory allocation efficiently? What trade-offs would you need to consider?

If I were to change my implementation to allow for an arbitrary number of piped commands and handle memory allocation properly, the incorporation of dynamic arrays through malloc for the commands attribute of the command_list_t structure, where the number of piped commands that can be processed within the shell can change at runtime. Instead of using CMD_MAX, this implementation will allow us to accept any number of piped commands arbitrailily. However, the introduction of dynamic arrays, it requires more downstream maintenance for the developer to keep tracking of freeing memory to avoid resource leaks, as the list of piped commands scales upwards. Considering that we are using malloc to create dynamic arrays, we might run into longer times to search for a particular piped command with linear time complexities. 